# DPO Training Configuration

# Base model and SFT checkpoint
base_model: "Qwen/Qwen2.5-0.5B-Instruct"
sft_model_path: "./models/checkpoints/qwen-sft/final"

# DPO hyperparameters
dpo:
  beta: 0.1  # KL penalty coefficient (tune: 0.1 - 0.5)
  max_seq_length: 1024
  max_prompt_length: 768
  max_target_length: 256

# RLAIF Stage 1: DPO on AI preferences
rlaif:
  output_dir: "./models/checkpoints/qwen-rlaif"
  learning_rate: 5.0e-5
  num_epochs: 1  # Start with 1, may increase to 2
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 16  # effective batch size = 64
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0

  # LoRA config (same as SFT)
  lora:
    r: 8
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
    bias: "none"
    task_type: "CAUSAL_LM"

  # Training data
  train_file: "./data/preferences/rlaif_train.jsonl"
  val_file: "./data/preferences/rlaif_val.jsonl"

# RLHF: DPO on human preferences only
rlhf:
  output_dir: "./models/checkpoints/qwen-rlhf"
  learning_rate: 2.0e-5  # Lower LR for small dataset
  num_epochs: 3  # More epochs due to small dataset
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4  # smaller effective batch size = 16
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0

  lora:
    r: 8
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
    bias: "none"
    task_type: "CAUSAL_LM"

  train_file: "./data/preferences/rlhf_train.jsonl"
  val_file: "./data/preferences/rlhf_val.jsonl"

# RLAIFâ†’RLHF Stage 2: DPO on human preferences starting from RLAIF
rlaif_to_rlhf:
  output_dir: "./models/checkpoints/qwen-rlaif-rlhf"
  init_model_path: "./models/checkpoints/qwen-rlaif/final"  # Start from RLAIF
  learning_rate: 2.0e-5
  num_epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0

  lora:
    r: 8
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
    bias: "none"
    task_type: "CAUSAL_LM"

  train_file: "./data/preferences/rlhf_train.jsonl"
  val_file: "./data/preferences/rlhf_val.jsonl"

# Training settings
training:
  fp16: true
  gradient_checkpointing: true
  logging_steps: 10
  eval_strategy: "steps"
  eval_steps: 50
  save_steps: 100
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

# Weights & Biases
wandb:
  project: "persuasion-rl-cs230"
  enabled: true
