{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Persuasion-RL: Exploratory Data Analysis\n",
        "\n",
        "This notebook provides exploratory analysis of the CMV and PersuasionForGood datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_jsonl(file_path):\n",
        "    \"\"\"Load JSONL file.\"\"\"\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "# Load training data\n",
        "train_data = load_jsonl('../data/processed/sft_train.jsonl')\n",
        "val_data = load_jsonl('../data/processed/sft_val.jsonl')\n",
        "test_data = load_jsonl('../data/processed/sft_test.jsonl')\n",
        "\n",
        "print(f\"Train: {len(train_data)} examples\")\n",
        "print(f\"Val: {len(val_data)} examples\")\n",
        "print(f\"Test: {len(test_data)} examples\")\n",
        "print(f\"Total: {len(train_data) + len(val_data) + len(test_data)} examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all data for analysis\n",
        "all_data = train_data + val_data + test_data\n",
        "\n",
        "# Extract metadata\n",
        "sources = [ex['metadata']['source'] for ex in all_data]\n",
        "response_tokens = [ex['metadata']['response_tokens'] for ex in all_data]\n",
        "context_tokens = [ex['metadata']['context_tokens'] for ex in all_data]\n",
        "\n",
        "print(\"Dataset Composition:\")\n",
        "source_counts = Counter(sources)\n",
        "for source, count in source_counts.items():\n",
        "    print(f\"  {source}: {count} ({count/len(all_data)*100:.1f}%)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
